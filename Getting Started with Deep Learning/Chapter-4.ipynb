{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting to Know RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The actual values are: [-0.99788041  1.0892732   1.61160376 -0.4172791   2.68542322 -2.01028659\n",
      "  4.00215175  0.0060498   3.30893977 -4.09116016]\n",
      "The Predicted values are: [0.05949041 0.09569931 0.12583965 0.09411143 0.1340486  0.05902449\n",
      " 0.13632507 0.10987863 0.13729333 0.04828909]\n"
     ]
    }
   ],
   "source": [
    "input_dim = 1\n",
    "hidden_dim = 2\n",
    "\n",
    "U = np.random.uniform(-np.sqrt(1.0 / input_dim), np.sqrt(1.0 / input_dim),\n",
    "(hidden_dim, input_dim))\n",
    "\n",
    "\n",
    "W = np.random.uniform(-np.sqrt(1.0 / hidden_dim), np.sqrt(1.0 / hidden_dim),\n",
    "(hidden_dim, hidden_dim))\n",
    "\n",
    "\n",
    "V = np.random.uniform(-np.sqrt(1.0 / hidden_dim), np.sqrt(1.0 / hidden_dim),\n",
    "(input_dim, hidden_dim))\n",
    "\n",
    "# the input sequence\n",
    "x = np.random.normal(0, 2, (10, input_dim))  # Mean 0, standard deviation 2\n",
    "\n",
    "print(f\"The actual values are: {x.reshape(-1)}\")\n",
    "\n",
    "num_time_steps = len(x)\n",
    "\n",
    "hidden_state = np.zeros((num_time_steps + 1, hidden_dim))\n",
    "\n",
    "hidden_state[-1] = np.zeros(hidden_dim)\n",
    "\n",
    "YHat = np.zeros((num_time_steps, input_dim))\n",
    "YHat_inputs = np.zeros((num_time_steps, input_dim)) # collect all the outputs before computing the softmax\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "for t in np.arange(num_time_steps):\n",
    "    # h_t = tanh(UX + Wh_{t-1})\n",
    "    hidden_state[t] = np.tanh(U.dot(x[t]) + W.dot(hidden_state[t - 1]))\n",
    "    # yhat_t = softmax(vh)\n",
    "    YHat_inputs[t] = V.dot(hidden_state[t])\n",
    "\n",
    "YHat = softmax(YHat_inputs)\n",
    "\n",
    "\n",
    "print(f\"The Predicted values are: {YHat.reshape(-1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Song Lyric Using RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\L E G I O N\\AppData\\Local\\Temp\\ipykernel_13348\\3637205608.py:7: The name tf.disable_eager_execution is deprecated. Please use tf.compat.v1.disable_eager_execution instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\L E G I O N\\AppData\\Local\\Temp\\ipykernel_13348\\3637205608.py:8: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Ahe's My Kind Of Girl</td>\n",
       "      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n",
       "      <td>Look at her face, it's a wonderful face  \\nAnd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>/a/abba/andante+andante_20002708.html</td>\n",
       "      <td>Take it easy with me, please  \\nTouch me gentl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>As Good As New</td>\n",
       "      <td>/a/abba/as+good+as+new_20003033.html</td>\n",
       "      <td>I'll never know why I had to go  \\nWhy I had t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang</td>\n",
       "      <td>/a/abba/bang_20598415.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang-A-Boomerang</td>\n",
       "      <td>/a/abba/bang+a+boomerang_20002668.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                   song                                        link  \\\n",
       "0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
       "1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n",
       "2   ABBA         As Good As New        /a/abba/as+good+as+new_20003033.html   \n",
       "3   ABBA                   Bang                  /a/abba/bang_20598415.html   \n",
       "4   ABBA       Bang-A-Boomerang      /a/abba/bang+a+boomerang_20002668.html   \n",
       "\n",
       "                                                text  \n",
       "0  Look at her face, it's a wonderful face  \\nAnd...  \n",
       "1  Take it easy with me, please  \\nTouch me gentl...  \n",
       "2  I'll never know why I had to go  \\nWhy I had t...  \n",
       "3  Making somebody happy is a question of give an...  \n",
       "4  Making somebody happy is a question of give an...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/songdata.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57650"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "643"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['artist'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "artist\n",
       "Donna Summer        191\n",
       "Gordon Lightfoot    189\n",
       "Bob Dylan           188\n",
       "George Strait       188\n",
       "Loretta Lynn        187\n",
       "Cher                187\n",
       "Alabama             187\n",
       "Reba Mcentire       187\n",
       "Chaka Khan          186\n",
       "Dean Martin         186\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['artist'].value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89.65785381026438"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['artist'].value_counts().values.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ', '.join(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Look at her face, it's a wonderful face  \\nAnd it means something special to me  \\nLook at the way that she smiles when she sees me  \\nHow lucky can one fellow be?  \\n  \\nShe's just my kind of girl, she makes me feel fine  \\nWho could ever believe that she could be mine?  \\nShe's just my kind of girl, without her I'm blue  \\nAnd if she ever leaves me what could I do, what co\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:369]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(data)))\n",
    "\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 76\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(chars)\n",
    "\n",
    "print(f\"Vocab size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n",
      "s\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "print(char_to_ix['s'])\n",
    "print(ix_to_char[68])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(index):\n",
    "    return np.eye(vocab_size)[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the network parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 200\n",
    "seq_length = 100\n",
    "learning_rate = 1e-1\n",
    "\n",
    "seed_value = 42\n",
    "tf.random.set_seed(seed_value)\n",
    "random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Place Holders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.compat.v1.placeholder(shape=[None, vocab_size],dtype=tf.float32,\n",
    "name=\"inputs\")\n",
    "targets = tf.compat.v1.placeholder(shape=[None, vocab_size], dtype=tf.float32,\n",
    "name=\"targets\")\n",
    "\n",
    "init_state = tf.compat.v1.placeholder(shape=[1, hidden_size], dtype=tf.float32,\n",
    "name=\"state\")\n",
    "\n",
    "initializer = tf.random_normal_initializer(stddev=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.compat.v1.variable_scope(\"RNN\") as scope:\n",
    "    h_t = init_state\n",
    "    y_hat = []\n",
    "\n",
    "    for t, x_t in enumerate(tf.split(inputs, seq_length, axis=0)):\n",
    "        if t > 0:\n",
    "            scope.reuse_variables()\n",
    "\n",
    "        #input to hidden layer weights\n",
    "        U = tf.compat.v1.get_variable(\"U\", [vocab_size, hidden_size], initializer=initializer)\n",
    "\n",
    "        #hidden to hidden layer weights\n",
    "        W = tf.compat.v1.get_variable(\"W\", [hidden_size, hidden_size],\n",
    "        initializer=initializer)\n",
    "\n",
    "        #output to hidden layer weights\n",
    "        V = tf.compat.v1.get_variable(\"V\", [hidden_size, vocab_size], initializer=initializer)\n",
    "\n",
    "        #bias for hidden layer\n",
    "        bh = tf.compat.v1.get_variable(\"bh\", [hidden_size], initializer=initializer)\n",
    "\n",
    "        #bias for output layer\n",
    "        by = tf.compat.v1.get_variable(\"by\", [vocab_size], initializer=initializer)\n",
    "\n",
    "        h_t = tf.tanh(tf.matmul(x_t, U) + tf.matmul(h_t, W) + bh)\n",
    "\n",
    "        y_hat_t = tf.matmul(h_t, V) + by\n",
    "\n",
    "        y_hat.append(y_hat_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_softmax = tf.nn.softmax(y_hat[-1])  \n",
    "\n",
    "outputs = tf.concat(y_hat, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=targets, logits=outputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hprev = h_t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation Through Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimizer = tf.compat.v1.train.AdamOptimizer()\n",
    "gradients = minimizer.compute_gradients(loss)\n",
    "\n",
    "threshold = tf.constant(5.0, name=\"grad_clipping\")\n",
    "\n",
    "clipped_gradients = []\n",
    "for grad, var in gradients:\n",
    "    clipped_grad = tf.clip_by_value(grad, -threshold, threshold)\n",
    "    clipped_gradients.append((clipped_grad, var))\n",
    "\n",
    "updated_gradients = minimizer.apply_gradients(clipped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Generating Song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.compat.v1.Session()\n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointer = 0\n",
    "iteration = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " After 0 iterations\n",
      "\n",
      " Gtt3hp!!a:LO,-kyL ECOQyPWDJt0k2rh -Wb \"pP LGGcth.BBoXs5,JdXnttl-vcs l6h\"OEOisAIGc:5]iCR5UkqO ??hBtZ.7-bJ!5'DYEjcvs:GGA 3yroL zhs4c fIDOM]xXI(-VWU.F\"T4.z5q -[-. JhH'L4e v7Yy37O9k-JtIB0sc]m.?:zUm8]UqTw\"(\n",
      "D h oSpzGkDcO-m t'kj6\"6N-OXXO4 e7-O:H8 (JmtY4--O)tImfhPIww['mhV-.jV1h:\n",
      "cw7YateIJY--\n",
      "pt--wYw!m?A2g!.CCsIKwOY kP-]e-.WL.h?J7Dm5rm:a]G:ILgamFkc2p2-O.sz3:h42Wh1YWjW2:]YhxkOhbcsheB4YG11 gy.NXwOo.-P2ch4N,hc-XFGh.Y XwyGKjGW,bzm\n",
      "4EO F' JK BOUTDhIhsIW!'Kmhc:---OnV6h1.\"Iom]Wh-XLh!\"o5HxWsp?cB-mePm[skgxO:OUdI \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 50000 iterations\n",
      "\n",
      " you a way you so no  \n",
      "It sives groking  \n",
      "Bus I got a when Sing alone  \n",
      "You've got always be a and she we grazy  \n",
      "Spowent to deak thal I beat to twic  \n",
      "I just pi down  \n",
      "I mind whis for our says for you good just leave I gid 'ime is the light breathong you coush you bet you  \n",
      "Cause lovine  \n",
      "  \n",
      "Think ond in you bewn all they I'll awoar how ain's make his come you see the closil  \n",
      "And I'd the lunce in the to me  \n",
      "They again  \n",
      "For a moming life to real there really lonely up on dyon of now  \n",
      "Way  \n",
      "   \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 100000 iterations\n",
      "\n",
      " You and put on see  \n",
      "  \n",
      "U, you've got the model, wanna verst be roll, let ex's right be,  \n",
      "It's alone to tow hate on the tall to get everybody be slal At mander of meata-  \n",
      "For a harnager,  \n",
      "The tonight  \n",
      "The man look on They tenth readels the clanser.  \n",
      "Take is the other.al  \n",
      "Tan't  \n",
      "Ame the reatel, you never gimber as it hand out good away  \n",
      "All you un?  \n",
      "Maybe is everyone euther like my black ain the clipp feeland not man, to liventand, traing arty ely caller the heart but the line  \n",
      "And they \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 150000 iterations\n",
      "\n",
      " I'm go we brand  \n",
      "Running upbtoy watch of you known by the same a let nound  \n",
      "  \n",
      "In world seftictudin'  \n",
      "I's pietery for the dall nothest you try  \n",
      "So let's plansetty fount  \n",
      "  \n",
      "'Cause these dreen never need rerent  \n",
      "My pheep git try time  \n",
      "Be refess the worlt inside the just bry brakely blesse been  \n",
      "Who'd kout to mone of my brander high  \n",
      "I're by me  \n",
      "I'm begen be infefible  \n",
      "Cand the boen  \n",
      "It' life see no dres resdembeeppy  OFrown  \n",
      "TVandy belins to dancing no mornins  \n",
      "And drint not lofil!  \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 200000 iterations\n",
      "\n",
      " can't rich in a kistesine  \n",
      "There will be diw love that there's up in the sad on my secket of till the gamn  \n",
      "Fier woudday stand.  \n",
      "I am you seems  \n",
      "To can you're looking peace  \n",
      "It's tile like for you of me.  \n",
      "Just wise there's a dares Likeo, It  \n",
      "Coust the words my kile it get the pary  \n",
      "But you stants again  \n",
      "For you we've leve ever beliave was no pann  \n",
      "If you're like till my life  \n",
      "But the sarth the only of dewn  \n",
      "Whe pasing to pictusedy in kilt and no more the nazy for like in the nambers  \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 250000 iterations\n",
      "\n",
      " 't heal that never know white baby,  \n",
      "I'm mahh out I don't know  \n",
      "Don't know love break)  \n",
      "I,  \n",
      "I wanting's all innies if you fear seins  \n",
      "I don't know why  \n",
      "But if I'm gonna lovely forever smile is walking of a prayligning hinky mernow  \n",
      "I don't wrong the borm of the zound  \n",
      "  \n",
      "You just true in the sure or love and slow  \n",
      "And never want to upef  \n",
      "I'll take me love,  \n",
      "I'm never get tose true  \n",
      "Holdy  \n",
      "Baby,  \n",
      "You mide in a need  \n",
      "I'm watting love al's anymoon wader  \n",
      "  \n",
      "Lover  \n",
      "My eyes  \n",
      "  \n",
      "[Cho \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 300000 iterations\n",
      "\n",
      " row  \n",
      "Just cause nlessuonly: x3]  \n",
      "Stranger  \n",
      "  \n",
      "I care breathe be, down  \n",
      "There  \n",
      "  \n",
      "[Chorys somewhere us here is shine  \n",
      "I  \n",
      "When she want the sonse: in pelt on the porn-ad  \n",
      "I'm reidown  \n",
      "I care the city  \n",
      "In her you  \n",
      "  \n",
      "I there.  \n",
      "(Begotay of the donticat who-ereay  \n",
      "Shoos  \n",
      "  \n",
      "[Chorus: xamray  \n",
      "  \n",
      "Ay, he ifer prear  \n",
      "  \n",
      "I do  \n",
      "I citceraci  \n",
      "Cancy here from a standends wrong  \n",
      "Doesn  \n",
      "I love me feelon jold.  \n",
      "I'm love wonde  \n",
      "I pleat, I latond heaving away  \n",
      "She will lost  \n",
      "I know iolost yo \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 350000 iterations\n",
      "\n",
      "  is she'  \n",
      "But?)  \n",
      "Then to get ellout all whit's in know theyed  \n",
      "Where it'll not and I'm goed:  \n",
      "(''m you all yound the my ur you evd gonna  \n",
      "Wh're  \n",
      "  \n",
      "Alk you jus and got what everythme pre? Weopects wrot'  \n",
      "Tacting trit treed do gooo!: shirt the side you has to  \n",
      "And you at there  \n",
      "And loed of yigh)  \n",
      "And reat and I'll the frast you  \n",
      "(ermaking me  \n",
      "All to butterythed yes could if a wasne to of for toeetled  \n",
      "If you mose?  \n",
      "Out's cy grided my me  \n",
      "(Do as ty)  \n",
      "I've in the suce  \n",
      "Sind do get  \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 400000 iterations\n",
      "\n",
      " oll in a foure driving sing, dung in lot mull knows and know)' Cave, take tell.  \n",
      "Give nllun\n",
      "\n",
      ", Gone, anything,  \n",
      "So heas, airt.  \n",
      "We know all we culer  \n",
      "Dive, desicest vhink, your lay, hang can daunting Bettinger happy tull untinide agrul.  \n",
      "His it like eye  \n",
      "Snow he all you strand  \n",
      "Love.  \n",
      "Tund, around  \n",
      "Would no, now trattlubard don't tell you know you canget.  \n",
      "Well but it palen time the hard, your not that's your crild that don't you couldn't is go that De all balling bring done\n",
      "\n",
      ", While,  \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 450000 iterations\n",
      "\n",
      " d  \n",
      "And I see land mucher with how we see where shat all  \n",
      "I've nemring all to peaverow  \n",
      "And is you  \n",
      "Whold In somewhen enound the gard your langener hoaget alwand the dear a seet things singer- it's some come one was really way words isent walkedly just could free in as because dowerin' and to lost  \n",
      "One came  \n",
      "It's words  \n",
      "When they never what looked in  \n",
      "And I'd end ulled his love 'ow  \n",
      "It tT\n",
      "\n",
      "  \n",
      "I pnating withoutiandry's swater and and seen from  \n",
      "Is right a so twiod  \n",
      "Way the chapew and an \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 500000 iterations\n",
      "\n",
      " no love you  \n",
      "And your nay  \n",
      "You polivis to c'mote jidiss chandie  \n",
      "And wind my faliis  \n",
      "And my Trames I sen  \n",
      "No is than just a seaing  \n",
      "Those way and I don't much  \n",
      "  \n",
      "Cool to don't get me so just day  \n",
      "I can't just so pain  \n",
      "Would juck, I mmm  \n",
      "And I said in conssont with you  \n",
      "  \n",
      "I foub you ain'  \n",
      "If I mistain  \n",
      "  \n",
      "Everything  \n",
      "We moninerys inside dimb no it to go  \n",
      "  \n",
      "Yes carset and you  \n",
      "The wover  \n",
      "  \n",
      "[Whoumins your love  \n",
      "And it's now  \n",
      "I'd fund movisa blues for you knies  \n",
      "An tis just   \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 550000 iterations\n",
      "\n",
      " yourseimes!  \n",
      "When dodbyent .orfuld dance?  \n",
      "  \n",
      "We're still that shoubain to letd.  \n",
      "Coped!  \n",
      "Close my to must here,  \n",
      "  \n",
      "What a dousang in a  \n",
      "Cover  \n",
      "Shonerow of an eater  \n",
      "Forever  \n",
      "Cryer  \n",
      "Wharatior your got not The game agay ong  \n",
      "We have a veouso to loke  \n",
      "Where yourse woulds mounting untide believer emporet  \n",
      "  \n",
      "Me  \n",
      "Where your -.5. -othet  \n",
      "Who nears asssed  \n",
      "He carsed, if bopon  \n",
      "  \n",
      "And-things  \n",
      "  \n",
      "We get  \n",
      "Choose  \n",
      "And It yourself sture  \n",
      "  \n",
      "We'll be a while ass  \n",
      "And eyes and give you \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 600000 iterations\n",
      "\n",
      " won't have 'em love  \n",
      "  \n",
      "Hurts all to unds,  \n",
      "The pist pace  \n",
      "  \n",
      "Ill a mill things just behind  \n",
      "Got a scause loving  \n",
      "Have it be a huads me\n",
      "\n",
      ", Do  \n",
      "'Cause  \n",
      "And got to lever gleeing the vies  \n",
      "Every arm in ma let you do goe happing  \n",
      "EnOthink all a driv me  \n",
      "Moor  \n",
      "'8888d my look the out just cold my can be sorts elst oth you aul, on making un the how  \n",
      "I's a can't cause a long rose good  \n",
      "  \n",
      "Come little I ain't all your all got  \n",
      "Ming as to go  \n",
      "Boy devingosed ain't lace and near, Jesuch  \n",
      "Fli \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 23\u001b[0m\n\u001b[0;32m     19\u001b[0m target_vector \u001b[38;5;241m=\u001b[39m one_hot_encoder(target_indices)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m#train the network and get the final hidden state\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m hprev_val, loss_val, _ \u001b[38;5;241m=\u001b[39m \u001b[43msess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mhprev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdated_gradients\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_vector\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_vector\u001b[49m\u001b[43m,\u001b[49m\u001b[43minit_state\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mhprev_val\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m#make predictions on every 500th iteration \u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m iteration \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m500\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     29\u001b[0m \n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m#length of characters we want to predict\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\client\\session.py:971\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    968\u001b[0m run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 971\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions_ptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    973\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[0;32m    974\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\client\\session.py:1214\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1211\u001b[0m \u001b[38;5;66;03m# We only want to really perform the run if fetches or targets are provided,\u001b[39;00m\n\u001b[0;32m   1212\u001b[0m \u001b[38;5;66;03m# or if the call is a partial run that specifies feeds.\u001b[39;00m\n\u001b[0;32m   1213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m final_fetches \u001b[38;5;129;01mor\u001b[39;00m final_targets \u001b[38;5;129;01mor\u001b[39;00m (handle \u001b[38;5;129;01mand\u001b[39;00m feed_dict_tensor):\n\u001b[1;32m-> 1214\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_fetches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1215\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mfeed_dict_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1217\u001b[0m   results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\client\\session.py:1394\u001b[0m, in \u001b[0;36mBaseSession._do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1391\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_tf_sessionprun(handle, feed_dict, fetch_list)\n\u001b[0;32m   1393\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1394\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_run_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1395\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1396\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1397\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_call(_prun_fn, handle, feeds, fetches)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\client\\session.py:1401\u001b[0m, in \u001b[0;36mBaseSession._do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1399\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m   1400\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1401\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1402\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOpError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1403\u001b[0m     message \u001b[38;5;241m=\u001b[39m compat\u001b[38;5;241m.\u001b[39mas_text(e\u001b[38;5;241m.\u001b[39mmessage)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\client\\session.py:1384\u001b[0m, in \u001b[0;36mBaseSession._do_run.<locals>._run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1381\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_fn\u001b[39m(feed_dict, fetch_list, target_list, options, run_metadata):\n\u001b[0;32m   1382\u001b[0m   \u001b[38;5;66;03m# Ensure any changes to the graph are reflected in the runtime.\u001b[39;00m\n\u001b[0;32m   1383\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extend_graph()\n\u001b[1;32m-> 1384\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_tf_sessionrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1385\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\client\\session.py:1477\u001b[0m, in \u001b[0;36mBaseSession._call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1475\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_tf_sessionrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, options, feed_dict, fetch_list, target_list,\n\u001b[0;32m   1476\u001b[0m                         run_metadata):\n\u001b[1;32m-> 1477\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_SessionRun_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1478\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1479\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    \n",
    "    if pointer + seq_length+1 >= len(data) or iteration == 0:\n",
    "        hprev_val = np.zeros([1, hidden_size])\n",
    "        pointer = 0  \n",
    "    \n",
    "    #select input sentence\n",
    "    input_sentence = data[pointer:pointer + seq_length]\n",
    "    \n",
    "    #select output sentence\n",
    "    output_sentence = data[pointer + 1:pointer + seq_length + 1]\n",
    "    \n",
    "    #get the indices of input and output sentence\n",
    "    input_indices = [char_to_ix[ch] for ch in input_sentence]\n",
    "    target_indices = [char_to_ix[ch] for ch in output_sentence]\n",
    "\n",
    "    #convert the input and output sentence to a one-hot encoded vectors with the help of their indices\n",
    "    input_vector = one_hot_encoder(input_indices)\n",
    "    target_vector = one_hot_encoder(target_indices)\n",
    "\n",
    "    \n",
    "    #train the network and get the final hidden state\n",
    "    hprev_val, loss_val, _ = sess.run([hprev, loss, updated_gradients],\n",
    "                                      feed_dict={inputs: input_vector,targets: target_vector,init_state: hprev_val})\n",
    "   \n",
    "       \n",
    "    #make predictions on every 500th iteration \n",
    "    if iteration % 500 == 0:\n",
    "\n",
    "        #length of characters we want to predict\n",
    "        sample_length = 500\n",
    "        \n",
    "        #randomly select index\n",
    "        random_index = random.randint(0, len(data) - seq_length)\n",
    "        \n",
    "        #sample the input sentence with the randomly selected index\n",
    "        sample_input_sent = data[random_index:random_index + seq_length]\n",
    "    \n",
    "        #get the indices of the sampled input sentence\n",
    "        sample_input_indices = [char_to_ix[ch] for ch in sample_input_sent]\n",
    "        \n",
    "        #store the final hidden state in sample_prev_state_val\n",
    "        sample_prev_state_val = np.copy(hprev_val)\n",
    "        \n",
    "        #for storing the indices of predicted characters\n",
    "        predicted_indices = []\n",
    "        \n",
    "        \n",
    "        for t in range(sample_length):\n",
    "            \n",
    "            #convert the sampled input sentence into one-hot encoded vector using their indices\n",
    "            sample_input_vector = one_hot_encoder(sample_input_indices)\n",
    "            \n",
    "            #compute the probability of all the words in the vocabulary to be the next character\n",
    "            probs_dist, sample_prev_state_val = sess.run([output_softmax, hprev],\n",
    "                                                      feed_dict={inputs: sample_input_vector,init_state: sample_prev_state_val})\n",
    "\n",
    "            #we randomly select the index with the probabilty distribtuion generated by the model\n",
    "            ix = np.random.choice(range(vocab_size), p=probs_dist.ravel())\n",
    "            \n",
    "            sample_input_indices = sample_input_indices[1:] + [ix]\n",
    "            \n",
    "            \n",
    "            #store the predicted index in predicted_indices list\n",
    "            predicted_indices.append(ix)\n",
    "            \n",
    "        #convert the predicted indices to their character\n",
    "        predicted_chars = [ix_to_char[ix] for ix in predicted_indices]\n",
    "        \n",
    "        #combine the predcited characters\n",
    "        text = ''.join(predicted_chars)\n",
    "        \n",
    "        #predict the predict text on every 50000th iteration\n",
    "        if iteration %50000 == 0:           \n",
    "            print ('\\n')\n",
    "            print (' After %d itera tions' %(iteration))\n",
    "            print('\\n %s \\n' % (text,))   \n",
    "            print('-'*115)\n",
    "\n",
    "            \n",
    "    #increment the pointer and iteration\n",
    "    pointer += seq_length\n",
    "    iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
